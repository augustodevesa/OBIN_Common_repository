# OB Integration Provisioning Integration troubleshooting for NOVUM> Technical assessmentOB integration & gOB Task:: NOVUM: gOB ElasticSearch Dashboards and WoW for gOB# Index1. References & Related Inf2. Objective, assumptions & current situation2.1 WHAT?2.2 WHY?2.3 HOW?3. Technical assessment4. gOB current troubleshooting solution4.1 gOB Call and SMS CDRs4.2 gOB run logs4.3 gOB fault alarms & performance alarms4.4 NGIN provided tools5. OB integrations current troubleshooting tools5.1 Splunk5.2 Voipmonitor6. Short-term troubleshooting solution for NOVUM7. Mid/long-term troubleshooting solution evolution for NOVUM8. Conclusions# 1. References & Related Info[Troubleshooting_Splunk_v3](https://docs.google.com/presentation/d/1Zw3oi2DK-KgRUzCSwi6dmA3pIrgh1EBPtwVdueS6kTE/edit#slide=id.p28)[Dashboards: Splunk Dashboards of MIA](https://10.253.1.11/en-US/app/search/dashboards)[TUGo KPI](https://10.253.1.11/en-US/app/tugo/basic_monitoring?earliest=-15m&latest=now)[TUGo Subscription KPI](https://10.253.1.11/en-US/app/tugo/subscriptions?earliest=-7d%40h&latest=now)[TUGo Provisioning Monitoring](https://10.253.1.11/en-US/app/tugo/provision_argentina)[TUGo Provisioning Form](https://10.253.1.11/en-US/app/tugo/provision_stats?earliest=-4d%40d&latest=%40d&form.rel_time=-0h&form.ob_selector=72410)[Trouble Shooting](https://10.253.1.11/en-US/app/tugo/tu_go_comm_flows)[TUGo Diagnostics/Alerts](https://10.253.1.11/en-US/app/tugo/alerts) [TUGo Diagnostics/Dashboards](https://10.253.1.11/en-US/app/tugo/dashboards)# 2. Objective, assumptions & current situation### 2.1 WHAT?In the case of TU the provisioning flow was managed e2e from gBE. Therefore it used to be SEEN Team who owned the control and troubleshooting of this flow.In the case of NOVUM the provisioning flow changes radically, therefore the way we work must change. Regaring OB Integration share the provisioning flow starts when ObProv component dispach the ACTIVATE service event towards rSDP and finish when gOB recieves the startCall and startSMSNotifications events.This means that we should be able to give mainly support about gOB provisioning status, but it shall be discused if we may also give support regaring SDP and OB provisioning status as a technical reference team for the OBs.~~~~Sobre flujos de provision, ¿Mantendremos responsabilidad e2e de los flujos de provision como legacy de SEEN?~~~~## 4. WHY?This document is intended to analyse current troubleshooting solution of OB-related arround provisinioning how to improve it and evolve it according to NOVUM requirements. Due to OB Integration is the main technical focalpoint team for the OBs regarding IPComms integration, we should be able give support outside NOVUM BE and SDP integration.~~~+ Que sucede con SDP, es parte importante de la integración pero es un componente cuya responsabilidad es de la OB, Nuestro punto de observación sería ObProv Component.  ¿Debemos crear nosotros las herramientas de troubleShooting en ObProv,  tal como hacemos en los servers OREJAS de gBE?~~~## 5. HOW?As well as It’s required to define what would be the way to troubleshoot issues in gOB, and in the OB,  for NOVUM since Splunk is not going to be available we would have to replicate dashboards.The place and/or tool where all the info generated by OB platforms like mSDP error log events, alarms should be available for the organization must be defined and agreed for NOVUM. An idea could be reuse TU TRIPAS server where accessing via SSH we can upload logs and indexing them by fluentd to ElasticSearch Core and visualize them throgh Kibana in case of NOVUM.## 6. Technical assessmentProvisioning current troubleshooting solution### Provisioning flow#### Source of information.**Provisoining examples:**IsCostumer RequestObProvSubscription Request**UnProvisoning****Examples:**### Provisioning fault & performance alarms### Provided tools###SplunkActually Splunk is a troubleshooting tool not only for OB-related stuff but for all systems in the e2e chain of TU service. Splunk is indexing the logs coming from not only gOB but also rest of the e2e systems like gBE servers (call control, sms control, history control, notification control, push server…) or TUCore (BES, AAA, Freeswitch…). So splunk is actually the most powerful troubleshooting tool we have in TU. Providing not only mechanism to perform complex queries, but also dashboards for monitoring the service or for troubleshooting, reports, alerts… and so on. Splunk full set of features is out of the scope of this document, but to what gOB and OB integration concerns, Splunk provides the required mechanism to troubleshoot provisioning & lifecycle flows against the OB, Call/SMS Notification APIs (with or without rSDP), e2e Call/SMS flows, and provide dashboards to monitor them and so on... Below some examples of Splunk utilities are provided:## Short-term troubleshooting solution for NOVUMThe short-term troubleshooting solution for gOB & OB integration in NOVUM will be composed by:### Service CDRsSplunk (which will be available at least until EOY 2017).Metrics for OB Provisioning Service via Grafana.Logs for OB Provisioning Service via Kibana.It’s a MUST for NOVUM to gather CDR files (Call and SMS) from OB NGIN platform.   An Splunk forwarder is configured in order sent the data to Splunk where it will be indexed accordingly. An splunk forwarder provides:+ Tagging of metadata (source, sourcetype, and host) + Configurable buffering + Data compression + SSL security + Use of any available network ports + Running scripted inputs locallyForwarders usually do not index the data, but rather forward the data to a Splunk deployment that does the indexing and searching. Splunk deployment details for TU are out of the scope of this document.Current TU Splunk solution for gOB CDRs and runlogs will be reused for NOVUM.  However there are some improvements that may be applied:We could remove gOB and CDRs sampling rules (see [4]) for the OBs where NOVUM is commercially launched.We should review the gOB components being indexed in Splunk for each OB. For example, OSIP module is not indexed in ARG and we may want to have it since it can be correlated with E2E correlator (e.g. for RBT queries).	E.g. Indexing in ARG…Since the OB integration and gOB solution is mostly shared by TU and NOVUM, Splunk can be still used in the short-term to check NOVUM related stuff. For example to check rSDP issues, NGIN issues or any other problem related to flows based on UNICA APIs… which in many cases could be common for TU and NOVUM. However, in the short-term we should be able to use NOVUM metrics and logs available in Grafana and Kibana respectively. Below is provided the High level architecture for metrics, monitoring, BI and CDRs in NOVUM: Source: https://www.draw.io/#G0B2Rh3dIZy7jrMFRKTW8yR2JvTlU~~~2017/05 At this point in time the architecture above is being defined and there are no specific dashboards created for NOVUM B2C in ARG. By the time the service is commercially launched, the required metrics and logs must be available.~~~Below an example of Grafana is provided (e.g. from NOVUM ARG B2B):Below an example of Kibana is provided:Kibana is actually providing the web interface where we connect to, but the solution is composed by a full suite of modules (https://www.elastic.co/products). Kibana is also known as ELK (Elasticsearch, Logstash & Kibana) or EFK (Elasticsearch, FluentId & Kibana) which are analogous to the solution provided by Splunk. Mid/long-term troubleshooting solution evolution for NOVUM### As mid-term and long-term evolutions, the following tracks should be analysed:[Mid-term] We should work on having our own dashboards and searches (or request adaptations for the existing ones at NOVUM B2C ARG Commercial Launch) in Grafana/Kibana. Basically considering the relevant information from the point of view of the integration with the OB or gOB. Metrics and logs from CCS, SCS or OB Provisioning service are really useful since those elements have direct interface with the OB and gOB. We could measure or check the status of:+ Provisioning Flow.+ rSDP (when it applies).+ Call/SMS Notification APIs performance.+ OB UNICA APIs performance (oProv, User Context, Notification API)...+ OB User life-cycle (suspensions, deactivations from OB…)[Mid-term] Since Splunk will be no longer available at some point in time, as mid-term evolution will be required to handle gOB CDR and run log files via Kibana. **For doing so, Tripas server must be configured accordingly in order to forward the files for being indexed and queryable from Kibana. Most likely configuring Beats or Logstash to feed up Elasticsearch engine and visualize the data from Kibana.**It would be required to define the way the information is going to be indexed. In case of ELK, it is based on a key-value approach and we would have to work in the definition of those keys. We should consider the main tools we are using in Splunk now for TU, and try to replicate them and improve them in Kibana. A really useful Splunk dashboard for example is the “Call Investigations”. We could try to create something similar in Kibana for us to have an easy way to check what happens for and specific call or SMS session in NOVUM.Now, for TU, we have Splunk dashboards differentiated per OB and per flow (e.g. incoming call) and we may need to generate something similar in NOVUM for the gOB and OB specifics. ### [Long-term] We could open a track to analyze whether to generate metrics from gOB being sent in real time to NOVUM backend as other systems do. However, first we need to:+ Validate if it is technically feasible.gOB is developed in C++. Confirm if the required Prometheus or Graphite libraries are available.Metrics are sent in real time, the impact on the VPN bandwidth should be checked too.gOB is deployed in SEE boards, but the connectivity with the NGIN is open with the FEP boards. It may be required a metrics aggregator in FEP (or any other solution) to overcome this problem....We may ask Huawei if they support a Metrics approach in the NGIN platform.Validate whether if it makes sense.¿What would be the benefit?¿Does it provide useful info with respect the one we already have by other means (CDRs, runlogs…)?Other systems in NOVUM backend (on the border with the OB like CCS, SCS or Provisioning) already generates metrics and EventLogs (CDRs). ¿We would provide value with gOB metrics?¿Would someone use this metrics?...+ Validate cost-benefit balance.	Metrics are normally intended to detect problems at short notice. In order to troubleshoot a problem later on you will not use metrics but logs or CDRs.  So, it would make sense for us if we were monitoring metrics on a daily basis or most likely it would be intended for NOC.[Mid-term] We could open a track to analyze whether to complement (or replace) current gOB CDR & run log approach, generating LogEvents when a Call/SMS session ends. It could be done sending LogEvents via JSON being consumed in NOVUM backed by the corresponding service. That way we could take advantage of the existing troubleshooting, monitoring and BI infrastructure in NOVUM backend (see picture above) to:Process gOB CDRs and run log info as part of the NOVUM BI via Kafka (Something could be do in Pentaho for example). CDR & runlog info would be stored in database.The backend service consuming this info could write the information received in a log which can be indexed to be visualized in Kibana. Actually, a wrapper or similar can be used to adapt the format if required.But again, we should first:+ Validate if it is technically feasible.Sending JSON CDRs via HTTP from gOB should not be a problem when rSDP is bypassed (no TPS limitation and need to send this info via CAll/SMS Notification API). We could use same connectivity already open for Call/SMS Notification API (between FEPs on NGIN and F5 in Miami) to send this info.We should consider the impact on the VPN bandwidth for sending this LogEvents. However we don’t need to send an HTTP request every time a Call/SMS session is closed. We could actually implement a gOB handler to merge information coming from multiple sessions, to compress all the info and to apply retry policies in case it is needed. BW usage can be optimized. gOB can also optimized the amount of information provided with respect current CDR and run logs which are not optimized in that sense.F5 should be able to route those LogEvents to right service in NOVUM backend....+ Validate whether if it makes sense.¿What would be the benefit?We already have current gOB CDRs. Would it replace them? Or complement them?¿Would BI use this LogEvents?…At first sight, this option looks more than reasonable and a great improvement with respect current solution for several reasons:This option would remove current dependencies with Huawei and the OB.This solution would be available to use from day 0, as soon as gOB is deployed. So, it can be used in Telco Integration as well and will be available of course at commercial launch. Current approach is really difficult to implement due to the dependencies with the OB and NGIn platform.The solution is based on JSON over HTTP which is a really flexible approach and a highly optimizable solution in term of BW consumption.The main concerns is the impact of this option on NGIN capacity, and the impact on the BW consumption. + Validate cost-benefit balance.## Conclusions & Next StepsIn the short-term the troubleshooting solution will be basically the same applied for TU, but also taking advantage of the metrics, logs and event generated by other system such as CCS, SCS or OB provisioning which will also provide very valuable information about the OB integration related stuff.We should provide feedback on how to consume the information from those services or create our own dashboards with the relevant info from gOB & OB integration perspective as mid-term evolution... for doing so, we should acquire some minimal knowledge of how Kibana and/or Grafana works. For the short-term, we will have to live (most likely) with the metrics and dashboards initially available.For the short-term we should have a look at Splunk sampling rules to see if we can soft them (or directly remove them) for those OBs where NOVUM is commercial. And we should validate if we are indexing all the relevant gOB components in Splunk.For the short-term we should have a look at Voipmonitor dashboards to see if makes sense to create new ones for NOVUM.With the changes included in gOB 2.0 for the E2E Correlator, all the flows being handled by gOB including provisioning and Auth API and including gOB components such as OSIP can be properly correlated and troubleshooted. So, there is no need to change gOB logic to meet any information gap for NOVUM.For the mid-term we should work on having current gOB CDRs and run log files (stored in Tripas Server) indexed in Kibana to replace current functionalities provided by splunk. At some in time Splunk will be no longer available (most likely).The previous item could be not required if we decide to implement the LogEvent approach based on JSON over HTTP in gOB side. If this option is finally implemented, and it is implemented before Splunk is turned off, we could not longer require current gOB CDR and run logs files in Tripas Server and rely directly on the LogEvents sent from gOB via HTTP. ----