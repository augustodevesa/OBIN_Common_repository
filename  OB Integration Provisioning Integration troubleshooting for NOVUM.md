# OB Integration Provisioning Integration troubleshooting for NOVUM> Technical assessmentOB integration & gOB Task:: NOVUM: gOB ElasticSearch Dashboards and WoW for gOB# Index1. References & Related Inf2. Objective, assumptions & current situation2.1 WHAT?2.2 WHY?2.3 HOW?3. Technical assessment4. gOB current troubleshooting solution4.1 gOB Call and SMS CDRs4.2 gOB run logs4.3 gOB fault alarms & performance alarms4.4 NGIN provided tools5. OB integrations current troubleshooting tools5.1 Splunk5.2 Voipmonitor6. Short-term troubleshooting solution for NOVUM7. Mid/long-term troubleshooting solution evolution for NOVUM8. Conclusions# 1. References & Related Info[Troubleshooting_Splunk_v3](https://docs.google.com/presentation/d/1Zw3oi2DK-KgRUzCSwi6dmA3pIrgh1EBPtwVdueS6kTE/edit#slide=id.p28)[Dashboards: Splunk Dashboards of MIA](https://10.253.1.11/en-US/app/search/dashboards)[TUGo KPI](https://10.253.1.11/en-US/app/tugo/basic_monitoring?earliest=-15m&latest=now)[TUGo Subscription KPI](https://10.253.1.11/en-US/app/tugo/subscriptions?earliest=-7d%40h&latest=now)[TUGo Provisioning Monitoring](https://10.253.1.11/en-US/app/tugo/provision_argentina)[TUGo Provisioning Form](https://10.253.1.11/en-US/app/tugo/provision_stats?earliest=-4d%40d&latest=%40d&form.rel_time=-0h&form.ob_selector=72410)[Trouble Shooting](https://10.253.1.11/en-US/app/tugo/tu_go_comm_flows)[TUGo Diagnostics/Alerts](https://10.253.1.11/en-US/app/tugo/alerts) [TUGo Diagnostics/Dashboards](https://10.253.1.11/en-US/app/tugo/dashboards)# 2. Objective, assumptions & current situation### 2.1 WHAT?In the case of TU the provisioning flow was managed e2e from gBE. Therefore it used to be SEEN Team who owned the control and troubleshooting of this flow.In the case of NOVUM the provisioning flow changes radically, therefore the way we work must change. Regaring OB Integration share the provisioning flow starts when ObProv component dispach the ACTIVATE service event towards rSDP and finish when gOB recieves the startCall and startSMSNotifications events.This means that we should be able to give mainly support about gOB provisioning status, but it shall be discused if we may also give support regaring SDP and OB provisioning status as a technical reference team for the OBs.~~~~Sobre flujos de provision, ¿Mantendremos responsabilidad e2e de los flujos de provision como legacy de SEEN?~~~~## 4. WHY?This document is intended to analyse current troubleshooting solution of OB-related arround provisinioning how to improve it and evolve it according to NOVUM requirements. Due to OB Integration is the main technical focalpoint team for the OBs regarding IPComms integration, we should be able give support outside NOVUM BE and SDP integration.~~~+ Que sucede con SDP, es parte importante de la integración pero es un componente cuya responsabilidad es de la OB, Nuestro punto de observación sería ObProv Component.  ¿Debemos crear nosotros las herramientas de troubleShooting en ObProv,  tal como hacemos en los servers OREJAS de gBE?~~~## 5. HOW?It’s required to define what would be the way to troubleshoot issues in gOB, and in the OB,  for NOVUM since (for example) Splunk will not be eventually available at some point in time and there will no dashboards for NOVUM. In case of NOVUM, they generate metrics provided in real time to systems like Grafana. And that is something that gOB does not provide now. ¿Could we do something similar in gOB?¿Does it makes sense for gOB to have a metric-like approach?We need to make sure that gOB is currently generating all the info that we need. We have to be self-sufficient having all the required info... Questions like ¿Did we troubleshoot something via Splunk and now we need to generate further info in gOB? must be answered as part of this assessment...The place and/or tool where all the info generated by gOB will be available for the organization must be defined and agreed for NOVUM. It may be server (like Tripas server in case of TU) where accessing via SSH we can "grep" logs or gather them or indexing them to be forwarded to other tools like Splunk in TU or Kibana in case of NOVUM.Definitely, gOB & OB integrations troubleshooting short-term solution must be defined. In case of having gaps now, they should be identified and been raised as soon as possible. As mid-term or long-term evolution a solution based on metrics and/or JSON events, or the integration with Grafana and/or Kibana must be analysed.## 6. Technical assessmentgOB current troubleshooting solution###gOB Call and SMS CDRsOne of the main sources of information from gOB are the call and SMS records written in CDR files when a call and SMS session ends. For further details about the format and content of these records see [1]. Some examples of call and SMS CDRs generated by gOB are provided below:**Call CDR examples:**Incoming Call…ServiceID="0100700300";CallType="incoming call";CallSessionId="0000005E-000102BF-00001000-393106E1-1494892798330708-1-72207";CallingParty="00541138993504";CalledParty="00541136393032";RedirectingParty="null";DomainTransferNumber="890111136393032#0024";CallResult="answer";CallEndpoint="cs";BEAction="Route_RouteLeg";CallDuration="1417568";Success="true";ResultCode="1005";E2ECorrelator="1.A0553BF934B45F0A.CT#82:541136393032";Time="2017-05-15T23:59:58Z";Source="gConnectOBsFTHv20";App originated call (a.k.a outgoing call)ServiceID="0100700300";CallType="outgoing call";CallSessionId="00000065-000102CA-00001000-3D5107D9-1494892797823344-1-72207";CallingParty="00541153347869";CalledParty="00541140230309";RedirectingParty="null";DomainTransferNumber="null";CallResult="answer";CallEndpoint="null";BEAction="null";CallDuration="8572";Success="true";ResultCode="9007";E2ECorrelator="395d383c-ddda-34e5-9df6-2f8021e10aa0.635902.CO#69:541153347869";Time="2017-05-15T23:59:57Z";Source="gConnectOBsOSIHv20";**SMS CDR examples:**SMS MO (a.k.a outgoing message from CS domain)  ServiceID="0100700300";MessageType="outgoing";SmsId="00000059-000202E2-00001000-222107E3-1494892799850029-2-72207";OriginalSmsId="null";Domain="cs";SendingParty="00542920541034";RecipientParty="00542920556509";PrimaryTwinNumber="null";Encoding="SMSC Default Alphabet";Fragmented="false";NumberOfFragments="null";Fragment="null";BEAction="null";MessageLength="11";Success="false";ResultCode="1619";E2ECorrelator="2.8A46C93573D5389D.SO#39:542920541034";Time="2017-05-15T23:59:59Z";Source="gConnectOBsSMPPvPERv20";App originated message (a.k.a outgoing message from SIP domain)ServiceID="0100700300";MessageType="outgoing";SmsId="00000065-000102D2-00001000-775107E8-1494892799607227-1-72207";OriginalSmsId="null";Domain="sip";SendingParty="00541128260661";RecipientParty="00541153361073";PrimaryTwinNumber="null";Encoding="utf-8";Fragmented="false";NumberOfFragments="null";Fragment="null";BEAction="null";MessageLength="53";Success="true";ResultCode="4001";E2ECorrelator="e9687a9d-0d71-438c-ffda-4fe029a31416.n3y4lt.MO#03:541128260661";Time="2017-05-15T23:59:59Z";Source="gConnectOBsSMHv20";SMS MT (a.k.a. incoming message)  ServiceID="0100700300";MessageType="incoming";SmsId="00000065-000102E2-00001000-2D6107E3-1494892799347095-1-72207";OriginalSmsId="null";Domain="cs";SendingParty="00541131798022";RecipientParty="00541136069349";PrimaryTwinNumber="null";Encoding="UCS2 (ISO/IEC-10646)";Fragmented="false";NumberOfFragments="null";Fragment="null";BEAction="Continue";MessageLength="53";Success="true";ResultCode="1617";E2ECorrelator="1.393B4A20238615EC.UN#69:541136069349";Time="2017-05-15T23:59:59Z";Source="gConnectOBsSMPPvPERv20";gOB run logs**gOB runtime log files**   are intended to be used during the normal operation of the service, even in the live environment during the business operation. These logs provide much less information than debug logs but they provide important information for troubleshooting.gOB logic is designed with specific success and error points in the service logic. Identifying the most critical success and error points the logic goes through during an SMS or Call handling session. gOB will write a new runtime log entry for each success and error point in the logic, including the corresponding SmsId or CallSessionId associated to this entry. Therefore, the runtime log files provide (for each SmsId and CallSessionId) a record with all the success and error points reached in the service logic during the call/SMS handling. This information is very useful for troubleshooting, since it’s possible to keep track of all logic steps for each SMS (i.e. SmsId) and call (i.e. CallSessionId) handled by gOB. For further details about the format and content of these files see [1]. …[CallSessionId/SmsId][E2ECorrelator][Time in microseconds since 1970-01-01 00:00:00 UTC][Time in milliseconds since the beginning of the call/SMS processing in the service] SUCCESS/ERROR <Success/Error Code>Some examples of call and SMS run log entries generated by gOB are provided below:gOBsFTH component logic for incoming IDP…And the corresponding run log written when the IDP is received by the service:0515 20:59:59 NOTICE gConnectOBsFTHv20-4096  | 000000000 [0f2b3bd2][0000005D-000102BE-00001000-375106BE-1494892799815635-1-72207] [1.A4B35B79E9137FC5.CT#75:unknown] [1494892799815756 us] [0.134 ms] SUCCESS 1001gOBsSMPPMT component logic for incoming MAP message...And the corresponding run log written when the IDP is received by the service:0515 20:59:59 NOTICE gConnectOBsSMPPMT-4096  | 000000000 [351be001][00000007-000102E9-00001000-0A01078E-1494892799846688-1-72410] [1494892799855488 us] [8.818 ms] SUCCESS 7047  ### gOB fault alarms & performance alarmsgOB can generate single fault alarms under certain problematic scenarios (such as database access errors, or internal communication errors on NGIN…). All of them have a clearance category ADMC (Automatically Detected Manually Cleared), so these alarms must be cleared manually by the service operator in the OB. Individual alarms will include E2E correlator of the flow where they are raised to ease troubleshooting.But apart from the fault alarms, NGIN can be also configured to raise performance alarms (a.k.a. KPI alarms) based on statistics counters incremented by gOB. KPI alarms are generated when some important statistic oversteps the pre-configured threshold (e.g. number of unsuccessful incoming calls). The KPI alarms have a clearance category ADAC (Automatically Detected Automatically Cleared). These alarms will be cleared automatically according to the recovery condition defined.For further details about gOB related alarms see [1].###NGIN provided toolsFault and KPI alarms, as well as gOB statistics counters, can be monitored via Huawei’s I2000 client following Huawei’s procedures. For further details about NGIN platform procedures to monitor alarms and statistics see [2]. Below you can find an example of the I2000 tool:Fault and KPIs alarms are mostly used by OB operation team to monitor the service and is not that critical in gOB service troubleshooting for us.  As part of Huawei’s iManager I2000 suite, Huawei also provides a tracing tool (iTrace client) for traffic tracing on NGIN:  iTrace is an important tool for gOB troubleshooting which let us take correlated call/SMS traces for protocols such as:+ CAMEL+ SIP+ HTTP+ SMPP+ MAP+ CS1+ (INAP)+ Or internal operations on NGIN platform like CDR records creation, database accesses or internal IDL messages.+ OB integrations current troubleshooting tools###SplunkActually Splunk is a troubleshooting tool not only for OB-related stuff but for all systems in the e2e chain of TU service. Splunk is indexing the logs coming from not only gOB but also rest of the e2e systems like gBE servers (call control, sms control, history control, notification control, push server…) or TUCore (BES, AAA, Freeswitch…). So splunk is actually the most powerful troubleshooting tool we have in TU. Providing not only mechanism to perform complex queries, but also dashboards for monitoring the service or for troubleshooting, reports, alerts… and so on. Splunk full set of features is out of the scope of this document, but to what gOB and OB integration concerns, Splunk provides the required mechanism to troubleshoot provisioning & lifecycle flows against the OB, Call/SMS Notification APIs (with or without rSDP), e2e Call/SMS flows, and provide dashboards to monitor them and so on... Below some examples of Splunk utilities are provided:###VoipmonitorVoIPmonitor is open source network packet sniffer with GUI for SIP, RTP, RTCP, webRTC and VoIP protocols running on linux. It is designed to monitor and troubleshoot quality of SIP VoIP calls but was expanded to support much more. Since all the data from all the elements in the environments are recorded and indexed in one GUI and DB, we can search for correlation and get all application events. It provides an e2e view of the service right up to the signaling reaches the OB border (OB SBC). Flow charts are created on demand to show the complete E2E service flow regardless of protocol. Further details about this tool are included in [3].## Short-term troubleshooting solution for NOVUM###The short-term troubleshooting solution for gOB & OB integration in NOVUM will be composed by:gOB CDRs and run logs (as is now for TU from gOB 2.0 version).Huawei’s NGIN iManager I2000, including iTrace tool (as is now for TU).Splunk (which will be available at least until EOY 2017).Voipmonitor (as is now for TU but we should consider to generate Voipmonitor dashboards for NOVUM).Metrics from Call Control Service (CCS), SMS Control Service (SCS) and OB Provisioning Service via Grafana.Logs from Call Control Service (CCS), SMS Control Service (SCS) and OB Provisioning Service via Kibana.gOB CDRs and runlogs were revisited as part of the E2E correlation support from gOB 2.0. With the changes included in gOB 2.0 for the E2E correlator, all the flows being handled by gOB including provisioning and Auth API and including gOB components such as OSIP can be properly correlated and troubleshooted. And same solution applies for NOVUM. Those changes are specified in [5], NESNGIN-14717 and NESNGIN-14156. It’s a MUST for NOVUM to gather gOB CDR files (Call and SMS) from OB NGIN platform.  So an OB launching NOVUM must be already configured to export gOB CDRs and run logs. In case of TU, gOB CDR files are gathered by mean of SFTP and the files are stored in a server (Tripas server). An Splunk forwarder is configured in order sent the data to Splunk where it will be indexed accordingly. An splunk forwarder provides:+ Tagging of metadata (source, sourcetype, and host) + Configurable buffering + Data compression + SSL security + Use of any available network ports + Running scripted inputs locallyForwarders usually do not index the data, but rather forward the data to a Splunk deployment that does the indexing and searching. Splunk deployment details for TU are out of the scope of this document.Current TU Splunk solution for gOB CDRs and runlogs will be reused for NOVUM.  However there are some improvements that may be applied:We could remove gOB and CDRs sampling rules (see [4]) for the OBs where NOVUM is commercially launched.We should review the gOB components being indexed in Splunk for each OB. For example, OSIP module is not indexed in ARG and we may want to have it since it can be correlated with E2E correlator (e.g. for RBT queries).	E.g. Indexing in ARG…Since the OB integration and gOB solution is mostly shared by TU and NOVUM, Splunk can be still used in the short-term to check NOVUM related stuff. For example to check rSDP issues, NGIN issues or any other problem related to flows based on UNICA APIs… which in many cases could be common for TU and NOVUM. However, in the short-term we should be able to use NOVUM metrics and logs available in Grafana and Kibana respectively. Below is provided the High level architecture for metrics, monitoring, BI and CDRs in NOVUM: Source: https://www.draw.io/#G0B2Rh3dIZy7jrMFRKTW8yR2JvTlU~~~2017/05 At this point in time the architecture above is being defined and there are no specific dashboards created for NOVUM B2C in ARG. By the time the service is commercially launched, the required metrics and logs must be available.~~~Below an example of Grafana is provided (e.g. from NOVUM ARG B2B):Below an example of Kibana is provided:Kibana is actually providing the web interface where we connect to, but the solution is composed by a full suite of modules (https://www.elastic.co/products). Kibana is also known as ELK (Elasticsearch, Logstash & Kibana) or EFK (Elasticsearch, FluentId & Kibana) which are analogous to the solution provided by Splunk. Mid/long-term troubleshooting solution evolution for NOVUM### As mid-term and long-term evolutions, the following tracks should be analysed:[Mid-term] We should work on having our own dashboards and searches (or request adaptations for the existing ones at NOVUM B2C ARG Commercial Launch) in Grafana/Kibana. Basically considering the relevant information from the point of view of the integration with the OB or gOB. Metrics and logs from CCS, SCS or OB Provisioning service are really useful since those elements have direct interface with the OB and gOB. We could measure or check the status of:+ Provisioning Flow.+ rSDP (when it applies).+ Call/SMS Notification APIs performance.+ OB UNICA APIs performance (oProv, User Context, Notification API)...+ OB User life-cycle (suspensions, deactivations from OB…)[Mid-term] Since Splunk will be no longer available at some point in time, as mid-term evolution will be required to handle gOB CDR and run log files via Kibana. **For doing so, Tripas server must be configured accordingly in order to forward the files for being indexed and queryable from Kibana. Most likely configuring Beats or Logstash to feed up Elasticsearch engine and visualize the data from Kibana.**It would be required to define the way the information is going to be indexed. In case of ELK, it is based on a key-value approach and we would have to work in the definition of those keys. We should consider the main tools we are using in Splunk now for TU, and try to replicate them and improve them in Kibana. A really useful Splunk dashboard for example is the “Call Investigations”. We could try to create something similar in Kibana for us to have an easy way to check what happens for and specific call or SMS session in NOVUM.Now, for TU, we have Splunk dashboards differentiated per OB and per flow (e.g. incoming call) and we may need to generate something similar in NOVUM for the gOB and OB specifics. ### [Long-term] We could open a track to analyze whether to generate metrics from gOB being sent in real time to NOVUM backend as other systems do. However, first we need to:+ Validate if it is technically feasible.gOB is developed in C++. Confirm if the required Prometheus or Graphite libraries are available.Metrics are sent in real time, the impact on the VPN bandwidth should be checked too.gOB is deployed in SEE boards, but the connectivity with the NGIN is open with the FEP boards. It may be required a metrics aggregator in FEP (or any other solution) to overcome this problem....We may ask Huawei if they support a Metrics approach in the NGIN platform.Validate whether if it makes sense.¿What would be the benefit?¿Does it provide useful info with respect the one we already have by other means (CDRs, runlogs…)?Other systems in NOVUM backend (on the border with the OB like CCS, SCS or Provisioning) already generates metrics and EventLogs (CDRs). ¿We would provide value with gOB metrics?¿Would someone use this metrics?...+ Validate cost-benefit balance.	Metrics are normally intended to detect problems at short notice. In order to troubleshoot a problem later on you will not use metrics but logs or CDRs.  So, it would make sense for us if we were monitoring metrics on a daily basis or most likely it would be intended for NOC.[Mid-term] We could open a track to analyze whether to complement (or replace) current gOB CDR & run log approach, generating LogEvents when a Call/SMS session ends. It could be done sending LogEvents via JSON being consumed in NOVUM backed by the corresponding service. That way we could take advantage of the existing troubleshooting, monitoring and BI infrastructure in NOVUM backend (see picture above) to:Process gOB CDRs and run log info as part of the NOVUM BI via Kafka (Something could be do in Pentaho for example). CDR & runlog info would be stored in database.The backend service consuming this info could write the information received in a log which can be indexed to be visualized in Kibana. Actually, a wrapper or similar can be used to adapt the format if required.But again, we should first:+ Validate if it is technically feasible.Sending JSON CDRs via HTTP from gOB should not be a problem when rSDP is bypassed (no TPS limitation and need to send this info via CAll/SMS Notification API). We could use same connectivity already open for Call/SMS Notification API (between FEPs on NGIN and F5 in Miami) to send this info.We should consider the impact on the VPN bandwidth for sending this LogEvents. However we don’t need to send an HTTP request every time a Call/SMS session is closed. We could actually implement a gOB handler to merge information coming from multiple sessions, to compress all the info and to apply retry policies in case it is needed. BW usage can be optimized. gOB can also optimized the amount of information provided with respect current CDR and run logs which are not optimized in that sense.F5 should be able to route those LogEvents to right service in NOVUM backend....+ Validate whether if it makes sense.¿What would be the benefit?We already have current gOB CDRs. Would it replace them? Or complement them?¿Would BI use this LogEvents?…At first sight, this option looks more than reasonable and a great improvement with respect current solution for several reasons:This option would remove current dependencies with Huawei and the OB.This solution would be available to use from day 0, as soon as gOB is deployed. So, it can be used in Telco Integration as well and will be available of course at commercial launch. Current approach is really difficult to implement due to the dependencies with the OB and NGIn platform.The solution is based on JSON over HTTP which is a really flexible approach and a highly optimizable solution in term of BW consumption.The main concerns is the impact of this option on NGIN capacity, and the impact on the BW consumption. + Validate cost-benefit balance.## Conclusions & Next StepsIn the short-term the troubleshooting solution will be basically the same applied for TU, but also taking advantage of the metrics, logs and event generated by other system such as CCS, SCS or OB provisioning which will also provide very valuable information about the OB integration related stuff.We should provide feedback on how to consume the information from those services or create our own dashboards with the relevant info from gOB & OB integration perspective as mid-term evolution... for doing so, we should acquire some minimal knowledge of how Kibana and/or Grafana works. For the short-term, we will have to live (most likely) with the metrics and dashboards initially available.For the short-term we should have a look at Splunk sampling rules to see if we can soft them (or directly remove them) for those OBs where NOVUM is commercial. And we should validate if we are indexing all the relevant gOB components in Splunk.For the short-term we should have a look at Voipmonitor dashboards to see if makes sense to create new ones for NOVUM.With the changes included in gOB 2.0 for the E2E Correlator, all the flows being handled by gOB including provisioning and Auth API and including gOB components such as OSIP can be properly correlated and troubleshooted. So, there is no need to change gOB logic to meet any information gap for NOVUM.For the mid-term we should work on having current gOB CDRs and run log files (stored in Tripas Server) indexed in Kibana to replace current functionalities provided by splunk. At some in time Splunk will be no longer available (most likely).The previous item could be not required if we decide to implement the LogEvent approach based on JSON over HTTP in gOB side. If this option is finally implemented, and it is implemented before Splunk is turned off, we could not longer require current gOB CDR and run logs files in Tripas Server and rely directly on the LogEvents sent from gOB via HTTP. 